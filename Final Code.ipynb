{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWlVIfCd3fpB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "892b7dfc-cb1c-4e03-ee39-738b2df0155f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from tabulate import tabulate\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial import distance\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from scipy import spatial\n",
        "import string\n",
        "import gensim.downloader as api\n",
        "import spacy\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = api.load(\"glove-wiki-gigaword-50\") #choose from multiple models https://github.com/RaRe-Technologies/gensim-data"
      ],
      "metadata": {
        "id": "SjNkIOIE7ODx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59da0662-55d3-449f-a2d1-f951dae8b2c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(s):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    s = s.translate(translator)\n",
        "    tokens = word_tokenize(s)\n",
        "    tokens = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    return ' '.join(stemmed_tokens)\n",
        "def read_and_preprocess(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "        text = file.read().replace('\\n', ' ')\n",
        "    return preprocess(text)\n",
        "\n",
        "file_paths = []\n",
        "preprocessed_texts = {}\n",
        "\n",
        "for file_path in file_paths:\n",
        "    preprocessed_texts[file_path] = read_and_preprocess(file_path)\n",
        "\n",
        "# TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(preprocessor=preprocess)\n",
        "\n",
        "# Fit and transform the corpus\n",
        "corpus = [\"Alexis leads. The others TRADE lines, hurrying through the apartment and SINGING about painting the town red. The music here is different from the overture or traffic number -- brasher in feel. The music swells, they’re out the door, and WE CUT TO...\",\n",
        "]\n",
        "\n",
        "# Append preprocessed texts to the corpus\n",
        "for preprocessed_text in preprocessed_texts.values():\n",
        "    corpus.append(preprocessed_text)\n",
        "\n",
        "# Compute TF-IDF matrix\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n"
      ],
      "metadata": {
        "id": "KM0LQPEH3_fF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "627777e0-ae23-4e45-ae85-60598ff9705e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 17) (<ipython-input-6-d4eb6a1aa39e>, line 17)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-d4eb6a1aa39e>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    file_paths = [\"Bentham EC.txt\", \"Bentham RU.txt”, “Burke JFH.txt”, “Burke LPI.txt”, “Burke LPP.txt”, “Burke LRC.txt”, “Burke RRF.txt”, “Burke SAT.txt”, “Burke SCA.txt”, “Burke SFI.txt”, “Burke SMR.txt”, “Cobden EIA.txt”, “Cobden HW.txt”, “Cobden IH.txt”, Dilke BE .txt”, Dilke GB.txt”, Dilke ID .txt”, Dilke PGB.txt”, Freeman CPE.txt”, “Freeman GEC.txt”, “Freeman GGB.txt”, “Freeman HS.txt”, “Freeman STI.txt”, Freeman STN.txt”, Froude HE.txt”, Froude OC.txt”, Hobhouse DR.txt”, Hobhouse LI.txt”, “Hobhouse ME.txt”, “Hobson CL.txt”, “Hobson EMC.txt”, “Hobson FD.txt”, “Hobson IS.txt”, “Hobson TIG.txt”, “Hobson WSA.txt”, “Macaulay HE.txt”, “Maine AL.txt”, “Maine VEW.txt”, “Martin HBC.txt”, “Martin HW.txt”, “Martineau BRI.txt”, “Martineau DI.txt”, “Martineau HE.txt”, “Martineau IPE.txt”, “Mill CI.txt”, “Mill CO.txt”, “Mill CR.txt”, “Mill HB.txt”, “Mill MI.txt”, “Mill MID.txt”, “Mill OL.txt”, “Naoroji EAR.txt”, “Naoroji IBR.txt”, “Naoroji PUI.txt”, “Seeley EE.txt”, “Seeley GB.txt”, “Sidgwick AM.txt”, “Sidgwick DE.txt”, “Sidgwick ME.txt”, “Smith WON.txt”, “Spencer FC.txt”, “Spencer MVS.txt”]\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(s):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    s = s.translate(translator)\n",
        "    tokens = word_tokenize(s)\n",
        "    tokens = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    return ' '.join(stemmed_tokens)\n",
        "def read_and_preprocess(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "        text = file.read().replace('\\n', ' ')\n",
        "    return preprocess(text)\n",
        "\n",
        "file_paths = [.   ]\"Bentham EC.txt\", \"Bentham RU.txt”, “Burke JFH.txt”, “Burke LPI.txt”, “Burke LPP.txt”, “Burke LRC.txt”, “Burke RRF.txt”, “Burke SAT.txt”, “Burke SCA.txt”, “Burke SFI.txt”, “Burke SMR.txt”, “Cobden EIA.txt”, “Cobden HW.txt”, “Cobden IH.txt”, Dilke BE .txt”, Dilke GB.txt”, Dilke ID .txt”, Dilke PGB.txt”, Freeman CPE.txt”, “Freeman GEC.txt”, “Freeman GGB.txt”, “Freeman HS.txt”, “Freeman STI.txt”, Freeman STN.txt”, Froude HE.txt”, Froude OC.txt”, Hobhouse DR.txt”, Hobhouse LI.txt”, “Hobhouse ME.txt”, “Hobson CL.txt”, “Hobson EMC.txt”, “Hobson FD.txt”, “Hobson IS.txt”, “Hobson TIG.txt”, “Hobson WSA.txt”, “Macaulay HE.txt”, “Maine AL.txt”, “Maine VEW.txt”, “Martin HBC.txt”, “Martin HW.txt”, “Martineau BRI.txt”, “Martineau DI.txt”, “Martineau HE.txt”, “Martineau IPE.txt”, “Mill CI.txt”, “Mill CO.txt”, “Mill CR.txt”, “Mill HB.txt”, “Mill MI.txt”, “Mill MID.txt”, “Mill OL.txt”, “Naoroji EAR.txt”, “Naoroji IBR.txt”, “Naoroji PUI.txt”, “Seeley EE.txt”, “Seeley GB.txt”, “Sidgwick AM.txt”, “Sidgwick DE.txt”, “Sidgwick ME.txt”, “Smith WON.txt”, “Spencer FC.txt”, “Spencer MVS.txt”]\n",
        "preprocessed_texts = {}\n",
        "\n",
        "for file_path in file_paths:\n",
        "    preprocessed_texts[file_path] = read_and_preprocess(file_path)\n",
        "\n",
        "# TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(preprocessor=preprocess)\n",
        "\n",
        "# Fit and transform the corpus\n",
        "corpus = [\"Alexis leads. The others TRADE lines, hurrying through the apartment and SINGING about painting the town red. The music here is different from the overture or traffic number -- brasher in feel. The music swells, they’re out the door, and WE CUT TO...\",\n",
        "]\n",
        "\n",
        "# Append preprocessed texts to the corpus\n",
        "for preprocessed_text in preprocessed_texts.values():\n",
        "    corpus.append(preprocessed_text)\n",
        "\n",
        "# Compute TF-IDF matrix\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "627777e0-ae23-4e45-ae85-60598ff9705e",
        "id": "Z0HrdI-U1q2_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 17) (<ipython-input-6-d4eb6a1aa39e>, line 17)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-d4eb6a1aa39e>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    file_paths = [\"Bentham EC.txt\", \"Bentham RU.txt”, “Burke JFH.txt”, “Burke LPI.txt”, “Burke LPP.txt”, “Burke LRC.txt”, “Burke RRF.txt”, “Burke SAT.txt”, “Burke SCA.txt”, “Burke SFI.txt”, “Burke SMR.txt”, “Cobden EIA.txt”, “Cobden HW.txt”, “Cobden IH.txt”, Dilke BE .txt”, Dilke GB.txt”, Dilke ID .txt”, Dilke PGB.txt”, Freeman CPE.txt”, “Freeman GEC.txt”, “Freeman GGB.txt”, “Freeman HS.txt”, “Freeman STI.txt”, Freeman STN.txt”, Froude HE.txt”, Froude OC.txt”, Hobhouse DR.txt”, Hobhouse LI.txt”, “Hobhouse ME.txt”, “Hobson CL.txt”, “Hobson EMC.txt”, “Hobson FD.txt”, “Hobson IS.txt”, “Hobson TIG.txt”, “Hobson WSA.txt”, “Macaulay HE.txt”, “Maine AL.txt”, “Maine VEW.txt”, “Martin HBC.txt”, “Martin HW.txt”, “Martineau BRI.txt”, “Martineau DI.txt”, “Martineau HE.txt”, “Martineau IPE.txt”, “Mill CI.txt”, “Mill CO.txt”, “Mill CR.txt”, “Mill HB.txt”, “Mill MI.txt”, “Mill MID.txt”, “Mill OL.txt”, “Naoroji EAR.txt”, “Naoroji IBR.txt”, “Naoroji PUI.txt”, “Seeley EE.txt”, “Seeley GB.txt”, “Sidgwick AM.txt”, “Sidgwick DE.txt”, “Sidgwick ME.txt”, “Smith WON.txt”, “Spencer FC.txt”, “Spencer MVS.txt”]\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(s):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    s = s.translate(translator)\n",
        "    tokens = word_tokenize(s)\n",
        "    tokens = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    return ' '.join(stemmed_tokens)\n",
        "def read_and_preprocess(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "        text = file.read().replace('\\n', ' ')\n",
        "    return preprocess(text)\n",
        "\n",
        "file_paths = [\"Bentham EC.txt\", \"Bentham RU.txt\", \"Burke JFH.txt\", \"Burke LPI.txt\", \"Burke LPP.txt\", \"Burke LRC.txt\",\n",
        "              \"Burke RRF.txt\", \"Burke SAT.txt\", \"Burke SCA.txt\", \"Burke SFI.txt\", \"Cobden EIA.txt\", \"Cobden HW.txt\",\n",
        "              \"Cobden IH.txt\", \"Dilke BE .txt\", \"Dilke GB.txt\", \"Dilke ID.txt\", \"Dilke PGB.txt\", \"Freeman CPE.txt\", \"Freeman GEC.txt\",\n",
        "              \"Freeman GGB.txt\", \"Freeman HS.txt\", \"Freeman STI.txt\", \"Freeman STN.txt\", \"Froude HE.txt\", \"Froude OC.txt\",\n",
        "              \"Hobhouse DR.txt\", \"Hobhouse LI.txt\", \"Hobhouse ME.txt\", \"Hobson CL.txt\", \"Hobson EMC.txt\", \"Hobson FD.txt\",\n",
        "              \"Hobson IS.txt\", \"Hobson TIG.txt\", \"Hobson WSA.txt\", \"Macaulay HE.txt\", \"Maine AL.txt\", \"Maine VEW.txt\",\n",
        "              \"Martin HBC.txt\", \"Martin HW.txt\", \"Martineau BRI.txt\", \"Martineau DI.txt\", \"Martineau HE.txt\", \"Martineau IPE.txt\",\n",
        "              \"Mill CI.txt\", \"Mill CO.txt\", \"Mill CR.txt\", \"Mill HB.txt\", \"Mill MI.txt\", \"Mill MID.txt\", \"Mill OL.txt\",\n",
        "              \"Naoroji EAR.txt\", \"Naoroji IBR.txt\", \"Naoroji PUI.txt\", \"Seeley EE.txt\", \"Seeley GB.txt\", \"Sidgwick AM.txt\",\n",
        "              \"Sidgwick DE.txt\", \"Sidgwick ME.txt\", \"Smith WON.txt\", \"Spencer FC.txt\", \"Spencer MVS.txt\"]\n",
        "preprocessed_texts = {}\n",
        "\n",
        "for file_path in file_paths:\n",
        "    preprocessed_texts[file_path] = read_and_preprocess(file_path)\n",
        "\n",
        "# TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(preprocessor=preprocess)\n",
        "\n",
        "# Fit and transform the corpus\n",
        "corpus = [\"Alexis leads. The others TRADE lines, hurrying through the apartment and SINGING about painting the town red. The music here is different from the overture or traffic number -- brasher in feel. The music swells, they’re out the door, and WE CUT TO...\",\n",
        "]\n",
        "\n",
        "# Append preprocessed texts to the corpus\n",
        "for preprocessed_text in preprocessed_texts.values():\n",
        "    corpus.append(preprocessed_text)\n",
        "\n",
        "# Compute TF-IDF matrix\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n"
      ],
      "metadata": {
        "id": "Quf0pMwg1t22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get vectors using the pre-trained GloVe model\n",
        "def get_vector(s):\n",
        "    tokens = word_tokenize(s)\n",
        "    vectors = []\n",
        "    for token in tokens:\n",
        "        if token in model.key_to_index:\n",
        "            vectors.append(model[token])\n",
        "    if vectors:\n",
        "        return np.sum(np.array(vectors), axis=0)\n",
        "    else:\n",
        "        return np.zeros_like(model[\"hello\"])  # Return a zero vector if no valid word vectors are found\n",
        "\n",
        "# Cosine similarity with normalized vectors\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    vec1_normalized = vec1 / np.linalg.norm(vec1)\n",
        "    vec2_normalized = vec2 / np.linalg.norm(vec2)\n",
        "    return np.dot(vec1_normalized, vec2_normalized)\n",
        "\n",
        "# Calculate similarities\n",
        "results = []\n",
        "for file_name, preprocessed_text in preprocessed_texts.items():\n",
        "    sim = cosine_similarity(get_vector(preprocessed_text), get_vector(preprocessed_texts[\"Spencer MVS.txt\"]))\n",
        "    results.append([file_name, \"Spencer MVS.txt\", sim])\n",
        "\n",
        "# Sort results based on similarity coefficients in descending order\n",
        "results.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "# Print table\n",
        "print(tabulate(results, headers=[\"Text File 1\", \"Text File 2\", \"Cosine Similarity\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiDDZNu739pV",
        "outputId": "a65ca71f-48f8-43f5-8570-7e96caaa742c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text File 1        Text File 2        Cosine Similarity\n",
            "-----------------  ---------------  -------------------\n",
            "Spencer MVS.txt    Spencer MVS.txt             1\n",
            "Hobhouse LI.txt    Spencer MVS.txt             0.997793\n",
            "Hobhouse DR.txt    Spencer MVS.txt             0.996716\n",
            "Hobson CL.txt      Spencer MVS.txt             0.99669\n",
            "Burke RRF.txt      Spencer MVS.txt             0.995558\n",
            "Mill CR.txt        Spencer MVS.txt             0.994071\n",
            "Bentham RU.txt     Spencer MVS.txt             0.993959\n",
            "Burke SFI.txt      Spencer MVS.txt             0.993842\n",
            "Burke SCA.txt      Spencer MVS.txt             0.993484\n",
            "Burke LPP.txt      Spencer MVS.txt             0.99325\n",
            "Mill OL.txt        Spencer MVS.txt             0.992402\n",
            "Mill CO.txt        Spencer MVS.txt             0.992293\n",
            "Burke SAT.txt      Spencer MVS.txt             0.992001\n",
            "Burke LPI.txt      Spencer MVS.txt             0.991734\n",
            "Spencer FC.txt     Spencer MVS.txt             0.991617\n",
            "Maine VEW.txt      Spencer MVS.txt             0.991202\n",
            "Burke JFH.txt      Spencer MVS.txt             0.991118\n",
            "Burke LRC.txt      Spencer MVS.txt             0.991068\n",
            "Hobhouse ME.txt    Spencer MVS.txt             0.990248\n",
            "Hobson TIG.txt     Spencer MVS.txt             0.989476\n",
            "Maine AL.txt       Spencer MVS.txt             0.988882\n",
            "Cobden EIA.txt     Spencer MVS.txt             0.988167\n",
            "Hobson FD.txt      Spencer MVS.txt             0.987828\n",
            "Sidgwick DE.txt    Spencer MVS.txt             0.987714\n",
            "Hobson IS.txt      Spencer MVS.txt             0.987618\n",
            "Sidgwick ME.txt    Spencer MVS.txt             0.986991\n",
            "Smith WON.txt      Spencer MVS.txt             0.985504\n",
            "Freeman GGB.txt    Spencer MVS.txt             0.9845\n",
            "Bentham EC.txt     Spencer MVS.txt             0.984072\n",
            "Hobson WSA.txt     Spencer MVS.txt             0.983968\n",
            "Martineau HE.txt   Spencer MVS.txt             0.983807\n",
            "Naoroji EAR.txt    Spencer MVS.txt             0.981736\n",
            "Mill HB.txt        Spencer MVS.txt             0.98152\n",
            "Cobden IH.txt      Spencer MVS.txt             0.980736\n",
            "Sidgwick AM.txt    Spencer MVS.txt             0.980491\n",
            "Mill MI.txt        Spencer MVS.txt             0.979256\n",
            "Seeley EE.txt      Spencer MVS.txt             0.979224\n",
            "Naoroji IBR.txt    Spencer MVS.txt             0.978981\n",
            "Macaulay HE.txt    Spencer MVS.txt             0.978343\n",
            "Hobson EMC.txt     Spencer MVS.txt             0.978316\n",
            "Martineau BRI.txt  Spencer MVS.txt             0.977001\n",
            "Martineau IPE.txt  Spencer MVS.txt             0.976458\n",
            "Mill MID.txt       Spencer MVS.txt             0.975369\n",
            "Cobden HW.txt      Spencer MVS.txt             0.974518\n",
            "Freeman GEC.txt    Spencer MVS.txt             0.973683\n",
            "Mill CI.txt        Spencer MVS.txt             0.97319\n",
            "Froude HE.txt      Spencer MVS.txt             0.97262\n",
            "Dilke GB.txt       Spencer MVS.txt             0.96976\n",
            "Froude OC.txt      Spencer MVS.txt             0.969558\n",
            "Dilke PGB.txt      Spencer MVS.txt             0.966601\n",
            "Naoroji PUI.txt    Spencer MVS.txt             0.963312\n",
            "Freeman CPE.txt    Spencer MVS.txt             0.960713\n",
            "Seeley GB.txt      Spencer MVS.txt             0.95952\n",
            "Dilke BE .txt      Spencer MVS.txt             0.958514\n",
            "Freeman HS.txt     Spencer MVS.txt             0.957394\n",
            "Martineau DI.txt   Spencer MVS.txt             0.952051\n",
            "Freeman STN.txt    Spencer MVS.txt             0.949988\n",
            "Freeman STI.txt    Spencer MVS.txt             0.949017\n",
            "Martin HBC.txt     Spencer MVS.txt             0.938831\n",
            "Martin HW.txt      Spencer MVS.txt             0.901352\n",
            "Dilke ID.txt       Spencer MVS.txt             0.87787\n"
          ]
        }
      ]
    }
  ]
}